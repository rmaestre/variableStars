---
title: "Dnu and Dr experiments"
author: "Roberto Maestre"
date: "03/26/2019"
output: github_document
---

```{r setup, include=FALSE}
library(variableStars)
library(data.table)
library(ggplot2)
library(RColorBrewer)
library(plotly)
library(keras)
```


# A simple MLP classification approach

We propose a MLP to train a NN by generating synthetic data sample given two patterns frequencies and a distance between them. This generated data (the periodicitiy) is the NN input and the output is the two frequency patterns tramsformed into an *one-hot-encode* vector normalized with a *Softmax activation*.

The loss function used by the NN is the categorical crossentropy, and is defined as:

$$H(y, \hat{y}) = \sum_i y_i \log\Big(\frac{1}{\hat{y}}\Big) = -\sum_i y_i \log \hat{y}$$

Thus, given a unseen periodicitiy, the NN will prodives the probabilities over the class oputput distribution. This probabilities plus the periodicitiy itself, can provide an accurate heuristic to find the frequency of the patterns that generate the periodiciticy.

# Synthetic Data generation

```{r, cache=F, echo=T, warning=FALSE}
trunc <-
  function(x, ..., prec = 1)
    base::trunc(x * 10 ^ prec, ...) / 10 ^ prec

experiment_number <- 1000
input_dim <- 10001
num_classes <-
  length(seq(from = 0.1, to = 10/0.0864, by = 1)) # Buckets of possible classes
  
if (F) {
  # Save to disk
  load(file = "~/Downloads/x_train.RData")
  load(file = "~/Downloads/x_test.RData")
  load(file = "~/Downloads/y_train.RData")
  load(file = "~/Downloads/y_test.RData")
} else {
  # Matrix to save genereated data
  m_xtrain <- matrix(nrow = experiment_number + 1, ncol = input_dim)
  m_ytrain <- matrix(nrow = experiment_number + 1, ncol = num_classes)
  # Loop generating data
  count <- 1
  for (experiment in seq(1:experiment_number)) {
    # Select experiment parameters
    dnu <- trunc(runif(1, 1, 10), prec = 4)
    dr <- trunc(runif(1, 0, dnu), prec = 4)
    # Debug info with experiment configuration
    if (count %% 250 == 0) {
      print(
        paste(
          "Experiment:",
          count,
          " | dnu:",
          dnu,
          " | dr:",
          dr,
          sep = ""
        )
      )
    }
    
    # Data generation
    dt <- generate_data_modes(
      deltaNu = dnu,
      deltaR = dr,
      nuRange = c(2.5, 10),
      numPoints=7
    )
    # Execute experiment
    result <- process(
      frequency = dt$data$frequency,
      amplitude = dt$data$amplitude,
      filter = "uniform",
      gRegimen = 0,
      maxDnu = 15,
      minDnu = 15,
      numFrequencies = ifelse(nrow(dt$data) < 30, 31, nrow(dt) + 1),
      dnuGuessError = -1,
      debug = F
    )
    
    # X data
    m_xtrain[count, ] <-
      result$fresAmps[[names(result$fresAmps)[1]]]$b
    # Y data
    m_ytrain[count, ] <-
      to_categorical(round(dnu / 0.0864, 3), num_classes) +
      to_categorical(round(dr / 0.0864, 3), num_classes)
    count <- count + 1
  }
  # Remove not valid or empty rows
  AA <- na.omit(m_xtrain)
  m_xtrain <- matrix(AA, nrow = nrow(AA) , ncol = ncol(AA))
  BB <- na.omit(m_ytrain)
  m_ytrain <- matrix(BB, nrow = nrow(BB) , ncol = ncol(BB))
  
  # Split train/test
  smp_size <- floor(0.75 * nrow(m_xtrain))
  set.seed(123)
  ind <- sample(seq_len(nrow(m_xtrain)), size = smp_size)
  
  # Prepare partition
  x_train <- m_xtrain[ind,]
  x_test  <- m_xtrain[-ind,]
  y_train <- m_ytrain[ind,]
  y_test  <- m_ytrain[-ind,]
  
  # Save to disk
  #save(x_train, file = "~/Downloads/x_train.RData")
  #save(x_test, file = "~/Downloads/x_test.RData")
  #save(y_train, file = "~/Downloads/y_train.RData")
  #save(y_test, file = "~/Downloads/y_test.RData")
}
```


# Deep NN

A deep NN with two layers is proposed as a multiclass classificacion problem.


## Data asserts and reshaping for NN

```{r, cache=F, echo=T, warning=FALSE}
# Asserts on data training and test
stopifnot(dim(x_train)[1] == dim(y_train)[1])
stopifnot(dim(x_test)[1] == dim(y_test)[1])
stopifnot(dim(x_train)[2] == dim(x_train)[2])
stopifnot(dim(y_test)[2] == dim(y_train)[2])

# Get input dimension for NN
input_dim <- dim(x_train)[2] # (The periodicitiy dimension)
# Calculate the number of classes

# Reshape data input for tensor
x_train <-
  array_reshape(x_train, c(nrow(x_train), ncol(x_train), 1))
x_test <- array_reshape(x_test, c(nrow(x_test), ncol(x_test), 1))
```


## NN architecture and compile

```{r, cache=T, echo=T, warning=FALSE}
# Create a 1d convolutional NN
model <- keras_model_sequential() %>%

  # Encoder
  layer_conv_1d(
    filters = 5,
    kernel_size = 5,
    activation = 'relu',
    input_shape = c(input_dim, 1)
  ) %>%
  layer_max_pooling_1d(pool_size = 3) %>%
  layer_dropout(0.5) %>%
  layer_batch_normalization() %>%

  layer_flatten() %>%
  layer_dense(num_classes, activation = 'softmax')

# Configure a model for categorical classification.
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adadelta(lr = 0.1),
  metrics = c("accuracy")
)
summary(model) # Plot summary
```

## NN train

```{r, cache=T, echo=T, warning=FALSE}
# Fit model
history <- model %>% fit(
  x_train,
  y_train,
  epochs = 150,
  batch_size =  250,
  validation_split = 0.2,
  shuffle = T
)
```
### Train history


```{r, cache=F, echo=T, warning=FALSE}
plot(history) +
  theme_bw()
```

## NN evaluation

### One test

#### Prepare one given row

```{r, cache=F, echo=T, warning=FALSE}
id <- 2
sx <- x_test[id,,1]
sy <- y_test[id,]
dim(sy) <- c(1, ncol(y_test))
sy_hat <- predict(model, array_reshape(sx, c(1,length(sx),1)))
```

#### Model evaluation is:

```{r, cache=F, echo=FALSE, warning=FALSE}
evaluate(model, array_reshape(sx, c(1,length(sx),1)), sy)
```

### All test

```{r, cache=F, echo=T, warning=FALSE}
evaluate(model, x_test, y_test)
```

#### Check the softmax and real y vectors

All vector prositions (periods frequency patters) must be the same

```{r, cache=F, echo=T, warning=FALSE}
which(sy==1)

sort(sy_hat, index.return=TRUE, decreasing=T)$ix[1:4]
```

## Manually test

```{r, cache=F, echo=T, warning=FALSE}
# One manually choosen id
id <- 36
x <- x_test[id,,1]
dim(x) <- c(1, length(x), 1) # Set correct tensor dimension
y_hat <- predict(model, x) # MLP predictions
#plot(t(y_hat))
y <- y_test[id,] # Get real y

# Show real y vs predicted y_hat
which(y == 1)
which(y_hat > 0.001) # Choose a probabilistic threshold
```


## Manually test on new data

```{r, cache=F, echo=T, warning=FALSE}
# Select experiment parameters
distance <- trunc(runif(1, 0, 10), prec = 4)
numFreqs <- 100
periodF <- runif(1, 0.1, 6)
periodS <- runif(1, 0.1, 6)
# Debug info with experiment configuration
if (T) {
  print(
    paste(
      " Distance:",
      distance,
      " | numFreqs:",
      numFreqs,
      " | 1ยบ period:",
      round(periodF, 3),
      " (",
      round(periodF / 0.0864, 3),
      "muHz)",
      " | 2ยบ period:",
      round(periodS, 3),
      " (",
      round(periodS / 0.0864, 3),
      "muHz)",
      sep = ""
    )
  )
}

# Data generation
dt <- generate_data(
  numFreqs = numFreqs,
  distance = distance,
  periodF = periodF,
  periodS = periodS,
  baseAMplitudeFirst = 10,
  baseAMplitudeSecond = 10,
  seed = NULL,
  freqOneRandRange = 0.1,
  freqTwoRandRange = 0.1,
  ampRandRange = 1.0
)
# Execute experiment
result <- process(
  frequency = dt$x,
  amplitude = dt$y,
  filter = "uniform",
  gRegimen = 0,
  maxDnu = 15,
  minDnu = 15,
  numFrequencies = ifelse(nrow(dt) < 30, 31, nrow(dt) + 1),
  dnuGuessError = -1,
  debug = F
)

# Plot periodicities
dt <- prepare_periodicities_dataset(result$fresAmps)
# Prepared MLP probabilities
prob_threshold <- 0.001
x <- matrix(result$fresAmps[[names(result$fresAmps)[1]]]$b, nrow = 1)

dim(x) <- c(1, length(x), 1)
y_hat <- predict(model, x)
dtNN <- data.frame("fInv"=which(y_hat>prob_threshold), 
                    "b"=y_hat[which(y_hat>prob_threshold)])
head(dtNN[order(dtNN$b, decreasing = T), ], 50)
```

## Plot periodicities and >prob_threshold frequencies from NN

```{r, cache=F, echo=T, warning=FALSE}
ggplot(aes(fInv, b), data=dt) +
  geom_line() +
  geom_point(data=dtNN, shape=3) +
  theme_bw()
```

